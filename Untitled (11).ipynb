{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f04d56-568d-4b6f-a008-0b3ead55c4ca",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5771a6e-f035-4c42-b0fc-0151b05bc5f6",
   "metadata": {},
   "source": [
    "Q1. Difference between Euclidean and Manhattan distance in KNN:\n",
    "\n",
    "Euclidean distance: It is the straight-line distance between two points in space. In KNN, it calculates the square root of the sum of squared differences between corresponding coordinates of points.\n",
    "Manhattan distance: Also known as L1 norm or taxicab distance, it is the sum of absolute differences between corresponding coordinates of points. In KNN, it considers only horizontal and vertical movements.\n",
    "The choice between Euclidean and Manhattan distance may affect the performance of KNN. Euclidean distance is more sensitive to variations in magnitude and can be influenced by outliers, while Manhattan distance is less sensitive to outliers and may perform better in the presence of noise or irrelevant features.\n",
    "\n",
    "Q2. Choosing the optimal value of k:\n",
    "\n",
    "The optimal value of k depends on the dataset. A small k may be sensitive to noise, while a large k may smooth out patterns.\n",
    "Techniques to determine optimal k include cross-validation, grid search, or using algorithms like elbow method to find a balance between bias and variance.\n",
    "\n",
    "Q3. Impact of distance metric choice:\n",
    "\n",
    "The choice of distance metric in KNN significantly influences the algorithm's performance. Different distance metrics are suitable for different types of data and underlying assumptions:\n",
    "\n",
    "Euclidean distance: It is sensitive to the magnitude of features and is appropriate when features are measured in similar scales. However, it can be influenced by outliers.\n",
    "\n",
    "Manhattan distance (L1 norm): This metric is less sensitive to outliers and can perform well in the presence of irrelevant features. It's suitable when the relationships between features are better represented by horizontal and vertical movements.\n",
    "\n",
    "Minkowski distance: A generalization that includes both Euclidean and Manhattan distances. The parameter 'p' in Minkowski distance allows you to tune the metric, with Euclidean distance corresponding to p=2 and Manhattan distance to p=1.\n",
    "\n",
    "The choice between these metrics depends on the characteristics of the data. If the dataset has features with different scales or contains outliers, Manhattan distance or Minkowski distance with an appropriate 'p' value may be a better choice. Experimenting with different metrics and validating their performance through cross-validation is often necessary to determine the most suitable distance metric for a particular dataset.\n",
    "\n",
    "Q4. Common hyperparameters and tuning in KNN:\n",
    "\n",
    "Number of neighbors (k): Influences model complexity.\n",
    "Weight function: Decides how neighbors contribute. Options include uniform weights (equal contribution) or distance weights (closer neighbors have more influence).\n",
    "Distance metric: Chooses the method for measuring distance.\n",
    "Algorithm: Specifies how neighbors are found (e.g., brute-force, KD tree, or Ball tree).\n",
    "Hyperparameters can be tuned using techniques like grid search or randomized search with cross-validation.\n",
    "\n",
    "Q5. Effect of training set size on KNN:\n",
    "\n",
    "Larger training sets generally provide more representative information, leading to better generalization.\n",
    "However, too large a dataset might slow down computation.\n",
    "Cross-validation can help find an optimal trade-off between training set size and model performance.\n",
    "Q6. Drawbacks of KNN:\n",
    "\n",
    "Computational cost: KNN can be computationally expensive, especially with large datasets.\n",
    "Sensitivity to irrelevant features: KNN might perform poorly with irrelevant or redundant features.\n",
    "Vulnerable to outliers: Outliers can significantly impact KNN.\n",
    "Curse of dimensionality: Performance can degrade as the number of features increases.\n",
    "To overcome these drawbacks, feature selection, dimensionality reduction, outlier detection, and algorithmic optimizations can be employed. Choosing appropriate distance metrics and normalizing features can also enhance performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
